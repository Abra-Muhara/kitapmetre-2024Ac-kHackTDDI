{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzWb5NUBjFmk",
        "outputId": "4afe00ab-ac08-4ba6-d613-e8becbe31d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                               text  label\n",
            "0  26418.0  Gerçekten sizin hikayelerinizi izleyerek mi ye...      0\n",
            "1  14473.0            @USER Çoook çok bi baklava bi sen zaten      0\n",
            "2  16107.0  1) Sn. DÜKEL; Atatürk'ün, Karma E. M. ile başl...      0\n",
            "3  45908.0  Konfederasyonumuzun Aile ve Sosyal Politikalar...      0\n",
            "4  12878.0  Hakemler tarih yazıyorlar / 9 kişiye karşı 3-2...      1\n",
            "        id                                               text  label\n",
            "0  41993.0    @USER Sayın başkanım bu şekilde devam inşallah👏      0\n",
            "1  23000.0  Herkes gevşekliği kadar duyar kasıyor,hayat bö...      0\n",
            "2  42478.0  Olgun ilişkisi olan arkadaş size en güzel hedi...      0\n",
            "3  21748.0  @USER @USER Burada atıp tutacağına o kötü koşu...      1\n",
            "4  13607.0   @USER İşte o onur dediğin sende yok sorun o işte      1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "# Dosya yolları\n",
        "train_path = 'twitter-veri-seti/train.csv'\n",
        "test_path = 'twitter-veri-seti/test.csv'\n",
        "validation_path = 'twitter-veri-seti/valid.csv'\n",
        "\n",
        "# Verileri yükleyelim\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "validation_df = pd.read_csv(validation_path)\n",
        "\n",
        "# Eğitim ve doğrulama verilerini birleştirelim\n",
        "\n",
        "# İlk birkaç satırı görelim\n",
        "print(train_df.head())\n",
        "print(test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Tokenizer oluştur ve eğit\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "\n",
        "# Metinleri sekanslara çevir ve pad et\n",
        "max_len = 50\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_df['text'])\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
        "X_val_seq = tokenizer.texts_to_sequences(validation_df['text'])\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
        "\n",
        "# Etiketleri encode et\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_df['label'])\n",
        "y_test = le.transform(test_df['label'])\n",
        "y_val = le.transform(validation_df['label'])\n"
      ],
      "metadata": {
        "id": "N0QAc1pwkkpa"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN,GRU, Dense, Dropout\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Modeli tanımla\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(GRU(128, return_sequences=False, kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJmKHYWxkq7b",
        "outputId": "ce9c9724-7bef-4d33-9c8f-a1ad2b66e287"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 50, 50)            6659950   \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, 128)               69120     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6729199 (25.67 MB)\n",
            "Trainable params: 6729199 (25.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# EarlyStopping callback oluştur\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Modeli eğit\n",
        "history = model.fit(X_train_pad, y_train, epochs=1, batch_size=32, validation_data=(X_val_pad, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Test seti üzerinde değerlendirme\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCQruQlHmfap",
        "outputId": "1756384f-f7e9-4a07-f513-d66873252d57"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1325/1325 [==============================] - 72s 53ms/step - loss: 0.3252 - accuracy: 0.8704 - val_loss: 0.3807 - val_accuracy: 0.8559\n",
            "277/277 [==============================] - 3s 9ms/step - loss: 0.2368 - accuracy: 0.9112\n",
            "Test Accuracy: 91.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_offensiveness(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    pad_seq = pad_sequences(seq, maxlen=max_len)\n",
        "    pred = model.predict(pad_seq)\n",
        "    return 1 if pred >= 0.5 else 0"
      ],
      "metadata": {
        "id": "yP7PQOOmoCol"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/offensive_language_rnn_model.h5')\n",
        "\n",
        "# Tokenizer'ı kaydet\n",
        "with open('/content/tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Etiket kodlayıcıyı kaydet\n",
        "with open('/content/label_encoder.pkl', 'wb') as handle:\n",
        "    pickle.dump(le, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "qEHzSuIQqBOz"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=0.0\n",
        "for i in range(len(validation_df[\"text\"])):\n",
        "  result += predict_offensiveness(validation_df[\"text\"][i]) == validation_df[\"label\"][i]"
      ],
      "metadata": {
        "id": "0Z3EreSYqUaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result / len(validation_df[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUN3xGMrvQor",
        "outputId": "2b36a7ba-6a0e-44de-ee3c-a012e1e1c117"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8559225512528473"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}
